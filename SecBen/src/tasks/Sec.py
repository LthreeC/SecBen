import torch
import numpy as np
import os
from lm_eval.base import Task, rf
from lm_eval.metrics import mean, bleu, chrf, ter
from src.tasks.utils import process_text
# from .zhutils import process_zhtext
from seqeval.metrics import f1_score as entity_score
from sklearn.metrics import f1_score, matthews_corrcoef, mean_squared_error

from src.metrics.BARTScore.bart_score import BARTScorer
import code_bert_score
import evaluate

from commutil import dbg

from transformers import AutoTokenizer, AutoModel

PROJECT_ROOT = os.getenv("PROJECT_ROOT")
dbg(PROJECT_ROOT)
# DATASET_PATH = os.path.join(PROJECT_ROOT, "data")

DATASET_PATH = "<DATA_PATH>"


SRC_PATH = os.path.join(PROJECT_ROOT, "src")


# TODO
_CITATION = """
@misc{xie2023pixiu,
      title={PIXIU: A Large Language Model, Instruction Data and Evaluation Benchmark for Finance}, 
      author={Qianqian Xie and Weiguang Han and Xiao Zhang and Yanzhao Lai and Min Peng and Alejandro Lopez-Lira and Jimin Huang},
      year={2023},
      eprint={2306.05443},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
"""


class Classification(Task):
    CALCULATE_MCC = True
    LOWER_CASE = True
    VERSION = 1
    EVAL_LAST_TURN = True

    def reformulate_turn_req(self, req, turn_request, turn):
        return req

    def has_training_docs(self):
        return True

    def has_validation_docs(self):
        return True

    def has_test_docs(self):
        return True

    def training_docs(self):
        return self.dataset["train"]

    def validation_docs(self):
        return self.dataset["validation"]

    def test_docs(self):
        return self.dataset["test"]

    def construct_requests(self, doc, ctx):
        """Uses RequestFactory to construct Requests and returns an iterable of
        Requests which will be sent to the LM.

        :param doc:
            The document as returned from training_docs, validation_docs, or test_docs.
        :param ctx: str
            The context string, generated by fewshot_context. This includes the natural
            language description, as well as the few shot examples, and the question
            part of the document for `doc`.
        """
        cont_request = rf.greedy_until(ctx, {"until": None})
        return cont_request

    def doc_to_decontamination_query(self, doc):
        return doc["text"]

    def doc_to_text(self, doc):
        # TODO: Format the query prompt portion of the document example.
        return doc["query"]

    def doc_to_target(self, doc):
        # TODO: Format the query prompt portion of the document example.
        return doc["answer"]

    def process_results(self, doc, results):
        gold: str = doc["choices"][doc["gold"]]
        if self.LOWER_CASE:
            gold = gold.lower()
        ini_result = results[0].strip()
        if self.LOWER_CASE:
            ini_result = ini_result.lower()

        result = None
        for choice in doc["choices"]:
            if self.LOWER_CASE:
                choice = choice.lower()
            if choice in ini_result:
                result = choice
                break
        if result is None:
            result = "missing"

        acc = 1.0 if gold == result else 0.0

        results = {
            "acc": acc,
            "missing": int(result == "missing"),
            "f1": (result, gold),
            "macro_f1": (result, gold),
        }

        if self.CALCULATE_MCC:
            results["mcc"] = (result, gold)

        return results

    def higher_is_better(self):
        metrics = {
            "acc": True,
            "f1": True,
            "macro_f1": True,
            "missing": False,
        }
        if self.CALCULATE_MCC:
            metrics["mcc"] = True
        return metrics

    def weighted_f1(self, items):
        preds, golds = zip(*items)
        labels = list(set(golds))
        preds = np.array(preds)
        golds = np.array(golds)
        f1 = f1_score(golds, preds, average="weighted", labels=labels)
        return f1

    def macro_f1(self, items):
        preds, golds = zip(*items)
        labels = list(set(golds))
        preds = np.array(preds)
        golds = np.array(golds)
        f1 = f1_score(golds, preds, average="macro", labels=labels)
        return f1

    def matthews_corrcoef(self, items):
        preds, golds = zip(*items)
        labels = {label: i for i, label in enumerate(list(set(golds)))}
        preds = [labels.get(pred, -1) for pred in preds]
        golds = [labels.get(gold, -1) for gold in golds]
        return matthews_corrcoef(golds, preds)

    def aggregation(self):
        metrics = {
            "acc": mean,
            "missing": mean,
            "f1": self.weighted_f1,
            "macro_f1": self.macro_f1,
        }
        if self.CALCULATE_MCC:
            metrics["mcc"] = self.matthews_corrcoef
        return metrics


class AbstractiveSummarization(Task):
    VERSION = 1
    DATASET_NAME = None
    EVAL_LAST_TURN = True

    def reformulate_turn_req(self, req, turn_request, turn):
        return req

    def has_training_docs(self):
        return False

    def has_validation_docs(self):
        return False

    def has_test_docs(self):
        return True

    def training_docs(self):
        return self.dataset["train"]

    def validation_docs(self):
        return self.dataset["validation"]

    def test_docs(self):
        return self.dataset["test"]

    def doc_to_text(self, doc):
        # TODO: Format the query prompt portion of the document example.
        return doc["query"]

    def doc_to_target(self, doc):
        return doc["answer"]

    def process_results(self, doc, results):
        return {
            "rouge1": (doc["answer"], results[0]),
            "rouge2": (doc["answer"], results[0]),
            "rougeL": (doc["answer"], results[0]),
            "bert_score_f1": (doc["answer"], results[0]),
            "bart_score": (doc["answer"], results[0]),
        }

    def higher_is_better(self):
        return {
            "rouge1": True,
            "rouge2": True,
            "rougeL": True,
            "bert_score_f1": True,
            "bart_score": True,
        }

    def construct_requests(self, doc, ctx):
        """Uses RequestFactory to construct Requests and returns an iterable of
        Requests which will be sent to the LM.

        :param doc:
            The document as returned from training_docs, validation_docs, or test_docs.
        :param ctx: str
            The context string, generated by fewshot_context. This includes the natural
            language description, as well as the few shot examples, and the question
            part of the document for `doc`.
        """
        cont_request = rf.greedy_until(ctx, {"until": None})
        return cont_request

    def rouge_score(self, items):
        golds, preds = zip(*items)
        # rouge = evaluate.load("rouge")
        # rouge = evaluate.load("metrics/rouge")
        rouge = evaluate.load(os.path.join(SRC_PATH, "metrics/rouge"))
        results = rouge.compute(predictions=preds, references=golds)
        return results

    def rouge1(self, items):
        results = self.rouge_score(items)
        return results["rouge1"]

    def rouge2(self, items):
        results = self.rouge_score(items)
        return results["rouge2"]

    def rougeL(self, items):
        results = self.rouge_score(items)
        return results["rougeL"]

    def bert_score(self, items):
        if getattr(self, "_cache_bertscore", None) is None:
            golds, preds = zip(*items)
            # bertscore = evaluate.load("evaluate-metric/bertscore")
            # bertscore = evaluate.load("metrics/bertscore")
            # dbg(os.path.join(SRC_PATH, "metrics/bertscore"))
            bertscore = evaluate.load(os.path.join(SRC_PATH, "metrics/bertscore"))
            
            # MEMO 
            self._cache_bertscore = bertscore.compute(
                predictions=preds,
                references=golds,
                # model_type="metrics/model/bert-base-multilingual-cased",
                model_type=os.path.join(SRC_PATH, "metrics/model/bert-base-multilingual-cased"),
            )
            return self._cache_bertscore
        else:
            return self._cache_bertscore

    def bert_score_f1(self, items):
        res = self.bert_score(items)
        return sum(res["f1"]) / len(res["f1"])

    def bart_score(self, items):
        golds, preds = zip(*items)
        # bart_scorer = BARTScorer(device="cuda", checkpoint="facebook/bart-large-cnn")
        bart_scorer = BARTScorer(device="cuda", checkpoint=os.path.join(SRC_PATH, "metrics/model/bart-large-cnn"))
        # bart_scorer.load(path="metrics/BARTScore/bart_score.pth")
        bart_scorer.load(path=os.path.join(SRC_PATH, "metrics/BARTScore/bart_score.pth"))
        # res = bart_scorer.score(srcs=preds, tgts=golds, batch_size=8)
        res = bart_scorer.score(srcs=preds, tgts=golds)
        return sum(res) / len(res)

    def aggregation(self):
        return {
            "rouge1": self.rouge1,
            "rouge2": self.rouge2,
            "rougeL": self.rougeL,
            "bert_score_f1": self.bert_score_f1,
            "bart_score": self.bart_score,
        } 



class CodeQA(Task):
    VERSION = 1
    DATASET_NAME = None
    EVAL_LAST_TURN = True

    def reformulate_turn_req(self, req, turn_request, turn):
        return req

    def has_training_docs(self):
        return False

    def has_validation_docs(self):
        return False

    def has_test_docs(self):
        return True

    def training_docs(self):
        return self.dataset["train"]

    def validation_docs(self):
        return self.dataset["validation"]

    def test_docs(self):
        return self.dataset["test"]

    def doc_to_text(self, doc):
        # TODO: Format the query prompt portion of the document example.
        return doc["query"]

    def doc_to_target(self, doc):
        return doc["answer"]

    def process_results(self, doc, results):
        return {
            "codebert_score_precision": (doc["answer"], results[0]),
            "codebert_score_recall": (doc["answer"], results[0]),
            "codebert_score_f1": (doc["answer"], results[0]),
            "codebert_score_f3": (doc["answer"], results[0]),
        }

    def higher_is_better(self):
        return {
            "codebert_score_precision": True,
            "codebert_score_recall": True,
            "codebert_score_f1": True,
            "codebert_score_f3": True,
        }

    def construct_requests(self, doc, ctx):
        """Uses RequestFactory to construct Requests and returns an iterable of
        Requests which will be sent to the LM.

        :param doc:
            The document as returned from training_docs, validation_docs, or test_docs.
        :param ctx: str
            The context string, generated by fewshot_context. This includes the natural
            language description, as well as the few shot examples, and the question
            part of the document for `doc`.
        """
        cont_request = rf.greedy_until(ctx, {"until": None})
        return cont_request
    
    def codebert_score_precision(self, items):
        # print("[DEBUG]----------: items", items)
        # print("[DEBUG]----------: *items", *items)
        golds, preds = zip(*items)
        # print("[DEBUG]----------: golds, preds", golds, preds)

        results = code_bert_score.score(cands=preds, refs=golds, lang='python') # TODOA 这个nthreads引起的程序不会正常关闭, 但好像也没什么影响
        averaged_results = [torch.mean(result).item() for result in results]
        self.precision = averaged_results[0]
        self.recall = averaged_results[1]
        self.f1 = averaged_results[2]
        self.f3 = averaged_results[3]
        # print("[DEBUG]----------: precision recall f1 f3", self.precision, self.recall, self.f1, self.f3)

        return self.precision

    def codebert_score_recall(self, items):
        return self.recall

    def codebert_score_f1(self, items):
        return self.f1

    def codebert_score_f3(self, items):
        return self.f3


    # 为了解决计算指标都是逆天调用函数，相出的以上下策
    def aggregation(self):
        return {
            "codebert_score_precision": self.codebert_score_precision,
            "codebert_score_recall": self.codebert_score_recall,
            "codebert_score_f1": self.codebert_score_f1,
            "codebert_score_f3": self.codebert_score_f3,
        }


class NER(Task):
    VERSION = 1
    DATASET_PATH = "chancefocus/flare-ner"
    DATASET_NAME = None
    EVAL_LAST_TURN = True

    def reformulate_turn_req(self, req, turn_request, turn):
        return req

    def has_training_docs(self):
        return True

    def has_validation_docs(self):
        return True

    def has_test_docs(self):
        return True

    def training_docs(self):
        return self.dataset["train"]

    def validation_docs(self):
        return self.dataset["validation"]

    def test_docs(self):
        return self.dataset["test"]

    def should_decontaminate(self):
        return True

    def doc_to_decontamination_query(self, doc):
        return doc["text"]

    def doc_to_text(self, doc):
        # TODO: Format the query prompt portion of the document example.
        return doc["query"]

    def construct_requests(self, doc, ctx):
        """Uses RequestFactory to construct Requests and returns an iterable of
        Requests which will be sent to the LM.

        :param doc:
            The document as returned from training_docs, validation_docs, or test_docs.
        :param ctx: str
            The context string, generated by fewshot_context. This includes the natural
            language description, as well as the few shot examples, and the question
            part of the document for `doc`.
        """
        cont_request = rf.greedy_until(ctx, {"until": None})
        return cont_request

    def doc_to_target(self, doc):
        return doc["answer"]

    def process_results(self, doc, results):
        text = doc["text"]
        pred = process_text(results[0], text)
        

        return {"entity_f1": (pred, doc["label"], results[0])}

    def higher_is_better(self):
        return {
            "entity_f1": True,
        }

    @classmethod
    def entity_f1(cls, items):
        preds, golds, _ = zip(*items)
        # preds = [[item.lower() for item in line] for line in preds] 
        # golds = [[item.lower() for item in line] for line in golds]
        f1 = entity_score(golds, preds)

        return f1

    def aggregation(self):
        return {
            "entity_f1": self.entity_f1,
        }




class Sec_Cls_4(Classification):
    DATASET_PATH = os.path.join(DATASET_PATH, "Cls_4")

class Sec_Cls_6(Classification):
    DATASET_PATH = os.path.join(DATASET_PATH, "Cls_6")
    
class Sec_Cls_8(Classification):
    DATASET_PATH = os.path.join(DATASET_PATH, "Cls_8")
    
class Sec_Cls_9(Classification):
    DATASET_PATH = os.path.join(DATASET_PATH, "Cls_9")
    
class Sec_Cls_15(Classification):
    DATASET_PATH = os.path.join(DATASET_PATH, "Cls_15")
    
class Sec_Cls_16(Classification):
    DATASET_PATH = os.path.join(DATASET_PATH, "Cls_16")
    
class Sec_Cls_18(Classification):
    DATASET_PATH = os.path.join(DATASET_PATH, "Cls_18")
    
class Sec_Cls_25(Classification):
    DATASET_PATH = os.path.join(DATASET_PATH, "Cls_25")
    
class Sec_Cls_27(Classification):
    DATASET_PATH = os.path.join(DATASET_PATH, "Cls_27")


class Sec_AS_1(AbstractiveSummarization):
    DATASET_PATH = os.path.join(DATASET_PATH, "AS_1")
    
class Sec_AS_3_1(AbstractiveSummarization):
    DATASET_PATH = os.path.join(DATASET_PATH, "AS_3_1")
    
class Sec_AS_3_2(AbstractiveSummarization):
    DATASET_PATH = os.path.join(DATASET_PATH, "AS_3_2")
    
class Sec_AS_3_3(AbstractiveSummarization):
    DATASET_PATH = os.path.join(DATASET_PATH, "AS_3_3")
    
class Sec_AS_3_4(AbstractiveSummarization):
    DATASET_PATH = os.path.join(DATASET_PATH, "AS_3_4")
    
class Sec_AS_10(AbstractiveSummarization):
    DATASET_PATH = os.path.join(DATASET_PATH, "AS_10")
    
class Sec_AS_12(AbstractiveSummarization):
    DATASET_PATH = os.path.join(DATASET_PATH, "AS_12")
    
class Sec_AS_14(AbstractiveSummarization):
    DATASET_PATH = os.path.join(DATASET_PATH, "AS_14")
    

class Sec_CQA_28(CodeQA):
    DATASET_PATH = os.path.join(DATASET_PATH, "CQA_28")

# NEW SHOT
class Sec_NER_29_cyner(NER):
    DATASET_PATH = os.path.join(DATASET_PATH, "NER_29_cyner")
    
class Sec_NER_29_cyner_TEST(NER):
    DATASET_PATH = os.path.join(DATASET_PATH, "NER_29_cyner_TEST")

class Sec_NER_30_aptner(NER):
    DATASET_PATH = os.path.join(DATASET_PATH, "NER_30_aptner")

class Sec_AS_31_TheHackerNews(AbstractiveSummarization):
    DATASET_PATH = os.path.join(DATASET_PATH, "AS_31_TheHackerNews")

class Sec_Cls_32_cve(Classification):
    DATASET_PATH = os.path.join(DATASET_PATH, "Cls_32_cve")

class Sec_Cls_33_email(Classification):
    DATASET_PATH = os.path.join(DATASET_PATH, "Cls_33_email")

class Sec_Cls_34_mitre(Classification):
    DATASET_PATH = os.path.join(DATASET_PATH, "Cls_34_mitre")

class Sec_Cls_35_web(Classification):
    DATASET_PATH = os.path.join(DATASET_PATH, "Cls_35_web")



class Sec_AS_3_4_TEST(AbstractiveSummarization):
    DATASET_PATH = os.path.join(DATASET_PATH, "AS_3_4_TEST")

class Sec_Cls_4_TEST(Classification):
    DATASET_PATH = os.path.join(DATASET_PATH, "Cls_4_TEST")

class Sec_CQA_28_TEST(CodeQA):
    # DATASET_PATH = "/root/Project/Project0Science/DATA/CQA_28_TEST"
    # DATASET_PATH = "/root/Project/Project0Science/ChengYuan/CQA_28_TEST"
    DATASET_PATH = os.path.join(DATASET_PATH, "CQA_28_TEST")


    


# if __name__ == '__main__':
#     true = [['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-malware', 'O']]
#     pred = [['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Malware', 'O']]

#     true = [[item.lower() for item in line] for line in true] 
#     pred = [[item.lower() for item in line] for line in pred]

#     dbg(pred)

#     dbg(entity_score(true, pred))